{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2be5065",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Sommaire<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#La-métrique-d'évaluation\" data-toc-modified-id=\"La-métrique-d'évaluation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>La métrique d'évaluation</a></span></li></ul></li><li><span><a href=\"#Chargement-des-ressources\" data-toc-modified-id=\"Chargement-des-ressources-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Chargement des ressources</a></span><ul class=\"toc-item\"><li><span><a href=\"#Chargement-du-workspace\" data-toc-modified-id=\"Chargement-du-workspace-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Chargement du workspace</a></span></li></ul></li><li><span><a href=\"#Développement-des-modèles\" data-toc-modified-id=\"Développement-des-modèles-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Développement des modèles</a></span><ul class=\"toc-item\"><li><span><a href=\"#Modèle-baseline\" data-toc-modified-id=\"Modèle-baseline-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Modèle baseline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cold-start-problem\" data-toc-modified-id=\"Cold-start-problem-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Cold start problem</a></span></li><li><span><a href=\"#Recherche-des-hyperparamètres\" data-toc-modified-id=\"Recherche-des-hyperparamètres-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Recherche des hyperparamètres</a></span></li><li><span><a href=\"#Analyse-des-résultats\" data-toc-modified-id=\"Analyse-des-résultats-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>Analyse des résultats</a></span></li><li><span><a href=\"#Enregistrement-des-hyperparamètres\" data-toc-modified-id=\"Enregistrement-des-hyperparamètres-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>Enregistrement des hyperparamètres</a></span></li></ul></li><li><span><a href=\"#Modèle-de-type-content-based\" data-toc-modified-id=\"Modèle-de-type-content-based-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Modèle de type content based</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cold-start-problem\" data-toc-modified-id=\"Cold-start-problem-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Cold start problem</a></span></li><li><span><a href=\"#Recherche-des-hyperparamètres\" data-toc-modified-id=\"Recherche-des-hyperparamètres-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Recherche des hyperparamètres</a></span></li><li><span><a href=\"#Analyse-des-résultats\" data-toc-modified-id=\"Analyse-des-résultats-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Analyse des résultats</a></span></li><li><span><a href=\"#Enregistrement-des-hyperparamètres\" data-toc-modified-id=\"Enregistrement-des-hyperparamètres-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Enregistrement des hyperparamètres</a></span></li></ul></li><li><span><a href=\"#Modèle-de-collaborative-filtering\" data-toc-modified-id=\"Modèle-de-collaborative-filtering-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Modèle de collaborative filtering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cold-start-problem\" data-toc-modified-id=\"Cold-start-problem-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Cold start problem</a></span></li><li><span><a href=\"#Recherche-des-hyperparamètres\" data-toc-modified-id=\"Recherche-des-hyperparamètres-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Recherche des hyperparamètres</a></span></li><li><span><a href=\"#Analyse-des-résultats\" data-toc-modified-id=\"Analyse-des-résultats-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>Analyse des résultats</a></span></li><li><span><a href=\"#Enregistrement-des-hyperparamètres\" data-toc-modified-id=\"Enregistrement-des-hyperparamètres-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>Enregistrement des hyperparamètres</a></span></li></ul></li></ul></li><li><span><a href=\"#Sélection-du-meilleur-modèle\" data-toc-modified-id=\"Sélection-du-meilleur-modèle-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Sélection du meilleur modèle</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comparaison-des-résultats\" data-toc-modified-id=\"Comparaison-des-résultats-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Comparaison des résultats</a></span></li><li><span><a href=\"#Enregistrement-du-modèle\" data-toc-modified-id=\"Enregistrement-du-modèle-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Enregistrement du modèle</a></span></li></ul></li><li><span><a href=\"#Analyse-du-modèle\" data-toc-modified-id=\"Analyse-du-modèle-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Analyse du modèle</a></span><ul class=\"toc-item\"><li><span><a href=\"#Chargement-du-modèle\" data-toc-modified-id=\"Chargement-du-modèle-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Chargement du modèle</a></span></li><li><span><a href=\"#Chargement-des-données-de-test\" data-toc-modified-id=\"Chargement-des-données-de-test-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Chargement des données de test</a></span></li><li><span><a href=\"#Evaluation-sur-le-jeu-de-test\" data-toc-modified-id=\"Evaluation-sur-le-jeu-de-test-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Evaluation sur le jeu de test</a></span></li></ul></li><li><span><a href=\"#Déploiement-du-MVP\" data-toc-modified-id=\"Déploiement-du-MVP-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Déploiement du MVP</a></span><ul class=\"toc-item\"><li><span><a href=\"#Déploiement-du-modèle-sur-Azure-Container-Instances\" data-toc-modified-id=\"Déploiement-du-modèle-sur-Azure-Container-Instances-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Déploiement du modèle sur Azure Container Instances</a></span></li><li><span><a href=\"#Test-du-modèle\" data-toc-modified-id=\"Test-du-modèle-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Test du modèle</a></span></li><li><span><a href=\"#Déploiement-du-service-de-prédiction\" data-toc-modified-id=\"Déploiement-du-service-de-prédiction-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Déploiement du service de prédiction</a></span></li><li><span><a href=\"#Test-du-service-de-recommandation\" data-toc-modified-id=\"Test-du-service-de-recommandation-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Test du service de recommandation</a></span></li><li><span><a href=\"#Test-du-MVP\" data-toc-modified-id=\"Test-du-MVP-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Test du MVP</a></span></li><li><span><a href=\"#Nettoyage-des-ressources\" data-toc-modified-id=\"Nettoyage-des-ressources-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Nettoyage des ressources</a></span></li></ul></li><li><span><a href=\"#Architecture-de-déploiement-automatisé\" data-toc-modified-id=\"Architecture-de-déploiement-automatisé-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Architecture de déploiement automatisé</a></span></li><li><span><a href=\"#Pour-aller-plus-loin\" data-toc-modified-id=\"Pour-aller-plus-loin-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Pour aller plus loin</a></span><ul class=\"toc-item\"><li><span><a href=\"#Compléter-les-blocs-manquants-de-l'architecture-MLOps\" data-toc-modified-id=\"Compléter-les-blocs-manquants-de-l'architecture-MLOps-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Compléter les blocs manquants de l'architecture MLOps</a></span><ul class=\"toc-item\"><li><span><a href=\"#Acquisition-des-données\" data-toc-modified-id=\"Acquisition-des-données-8.1.1\"><span class=\"toc-item-num\">8.1.1&nbsp;&nbsp;</span>Acquisition des données</a></span></li><li><span><a href=\"#Monitoring-des-données\" data-toc-modified-id=\"Monitoring-des-données-8.1.2\"><span class=\"toc-item-num\">8.1.2&nbsp;&nbsp;</span>Monitoring des données</a></span></li></ul></li><li><span><a href=\"#Ajout-et-automatisation-des-tests\" data-toc-modified-id=\"Ajout-et-automatisation-des-tests-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Ajout et automatisation des tests</a></span></li><li><span><a href=\"#Analyse-de-la-stabilité-temporelle\" data-toc-modified-id=\"Analyse-de-la-stabilité-temporelle-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Analyse de la stabilité temporelle</a></span></li><li><span><a href=\"#Pousser-l'expérimentation-ML\" data-toc-modified-id=\"Pousser-l'expérimentation-ML-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Pousser l'expérimentation ML</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cdf066",
   "metadata": {
    "gather": {
     "logged": 1635146389029
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from P9_02_scripts.datasets import *\n",
    "from P9_02_scripts.models import *\n",
    "from notebook import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0935a848",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Ce notebook fait suite au précédent notebook `modélisation.ipynb`. Nous commencerons par tester et évaluer différents modèles de recommandation. Nous allons ensuite les comparer et sélectionner le plus performant. Enfin nous déployerons notre système de recommandation.\n",
    "\n",
    "Le but de ce projet est de créer au plus vite un MVP qui pourra être présenté à l'équipe et à des utilisteurs. Nous n'allons donc pas rechercher le modèle de recommandation le plus performant.\n",
    "\n",
    "Nous nous sommes plutôt concentré sur la création et l'implémentation de notre architecture MLOps qui nous a permit d'obtenir rapidement une première version du MVP. Nous pourrons ainsi rapidement itérer sur de futurs versions. De plus, la distribution des données du dataset que nous utilisons ici peut être très différente de celle que nous aurons dans la réalité (articles différents etc...). Cela a été un argument de plus pour nous concentrer sur la mise en place de notre architecture qui nous permettra de rapidement expérimenter et déployer des modèles avec nos futures données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294ee70",
   "metadata": {},
   "source": [
    "## La métrique d'évaluation\n",
    "\n",
    "Notre système de recommandation a pour but de présenter du contenus pertinents à nos utilisateurs. La question est de savoir comment mesurer l'efficacité de ce système.\n",
    "\n",
    "Il existe plusieurs types de métriques ([en savoir plus](https://github.com/microsoft/recommenders/tree/main/examples/03_evaluate)). Parmis elles, on retrouve :\n",
    "- `Rating metrics` : Nous allons mesurer la performance du système à prédire les notations des articles (RMSE etc...). Ces métriques sont intrasèques aux modèle et ne sont donc pas adaptés à tous les modèles. De plus, elles sont plus difficilement interprétables car elle ne dépendent pas directement de la pertinance des recommandations faites à l'utilisateur.\n",
    "- `Ranking metrics` : Permettent de mesurer la pertinance des recommandations pour les utilisateurs (precision, recall etc...). Ces métriques sont des mesure extrinsèques et peuvent être adaptées à tous les modèles. Elle sont facilement interprétables mais peuvent être très couteuse en ressources.\n",
    "- `Business metrics` : Les précédentes métriques pourraient pousser un modèle à recommander des articles dit \"pièges à clicks\" et qui ont par exemple des titres très accrocheurs. Cela pourrait faire baisser notre taux de rétention ainsi que le temps passé par les utilisateurs sur notre site. On peux alors utiliser des métriques comme le temps moyen passé par un utilisateur sur notre site et chercher ainsi à l'améliorer.\n",
    "\n",
    "Les `Business metrics` semblent être les plus intéressantes de notre point de vue. Il nous semble cependant un peu tôt pour en sélectionner une, surtout dans le cadre de ce MVP. Nous allons donc prendre une `Ranking metrics` inspirée du kernel Kaggle suivant : [Recommender Systems in Python 101](https://www.kaggle.com/gspmoreira/recommender-systems-in-python-101#Evaluation). Il s'agit de la métrique `Recall@5` qui va mesurer la proportion de contenu pertinent sur 5 recommandations effectuées parmis un échantillon de 101 articles.\n",
    "\n",
    "Voici son algorithme :\n",
    "- Pour chaque article cliqué par un utilisateur :\n",
    "\t- On crée une liste de 100 articles jamais cliqués par cet utilisateur (vrais négatifs).\n",
    "\t- On ajoute l’article cliqué par l’utilisateur dans la liste.\n",
    "\t- On demande au modèle de prédire 5 recommandations parmi ces 101 articles.\n",
    "\t- `user_article_recall` = l’article est dans les 5 recommandations.\n",
    "- `Recall@5` = moyenne de tous les `user_article_recall`.\n",
    "\n",
    "Cette algorithme est un bon compromis entre rapidité d'évaluation et interprétabilité. En effet, la partie la plus gourmande en ressources est la création des listes de 100 articles jamais cliqués par les utilisateurs (vrais négatifs). Or cette partie à déjà été réalisée lors de l'ajout de vrais négatifs dans les jeux de notations des articles créés dans le pipeline de transformation des données.\n",
    "\n",
    "Dans une première version de cet algorithme, nous avons échantillonné 100 articles parmis tous ceux disponibles dans le jeu de données. Dans cette version, on obtenait des scores `Recall@5` proches de 1 simplement en sélectionnant les 5 articles les plus récents. Il s'agissait donc d'un algorithme trop naïf et facilement contournable. L'astuce a été d'échantillonner 100 articles parmis ceux qui ont récemment été cliqués par d'autres utilisateurs et qui ont donc récemment suscité un intérêt. Cela a permis de rendre l'algorithme de calcul du `Recall@5` beaucoup plus robuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7c95f",
   "metadata": {},
   "source": [
    "# Chargement des ressources\n",
    "\n",
    "Nous allons charger les ressources Azure qui vont nous permettre d'exécuter nos scripts d'entrainement sur des clusters de calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ec754",
   "metadata": {},
   "source": [
    "## Chargement du workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc6fece",
   "metadata": {
    "gather": {
     "logged": 1635146405042
    }
   },
   "outputs": [],
   "source": [
    "# On charge l’espace de travail Azure Machine Learning existant\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d257416",
   "metadata": {},
   "source": [
    "# Développement des modèles\n",
    "\n",
    "Nous allons ici développer et expérimenter divers modèles de recommandation :\n",
    "\n",
    "<img src=\"./data/img/archi_dev.png\" alt=\"Développement et expérimentation\" style=\"width:800px;\"/>\n",
    "<p style=\"text-align: center; text-decoration: underline;\">Développement et expérimentation</p>\n",
    "\n",
    "Dans le précédent notebook, nous avons réalisé les blocs `Data analysis` et `Data transform`. Dans cette section, nous allons voir les blocs `Model training` et `Model evaluation`. Nous allons itérer ces opérations sur divers modèles en recherchant les meilleurs hyperparamètres. Nous allons ensuite sauvegarder les scripts d'entrainement ainsi que les meilleur hyperparamètres de chaque modèle dans notre repository Github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eca3c0",
   "metadata": {},
   "source": [
    "## Modèle baseline\n",
    "\n",
    "Nous allons commencer par tester 3 modèles naïfs qui vont nous servir de base de comparaison avec les autres modèles :\n",
    "1. Recommande les 5 articles les plus récents.\n",
    "2. Recommande les 5 articles les plus populaires (ceux qui ont le plus de clicks).\n",
    "3. Recommande 5 articles au hasard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a96819",
   "metadata": {},
   "source": [
    "### Cold start problem\n",
    "\n",
    "Le cold start problem est le nom donnée aux 2 problématiques suivantes :\n",
    "- Quoi recommandé à un nouvel utilisateur ?\n",
    "- A qui recommander un nouvel article ?\n",
    "\n",
    "Ce modèle ne sera pas confronté au cold start problem puisqu'il ne prend en compte que les informations des profils des articles. Or lorsque l'on va ajouter un nouvel article, celui-ci aura déjà assez d'information dans son profil pour qu'il soit exploitable par le modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6288d037",
   "metadata": {},
   "source": [
    "### Recherche des hyperparamètres\n",
    "\n",
    "Nos 3 types de recommandation sont implémentés en tant qu'hyperparamètre dans la classe `BaselineRecommender`. Nous allons les tester et ne conserver que la meilleure version pour notre baseline.\n",
    "\n",
    "Nous allons tester toutes les combinaisons de manière exhaustive en utilisant une recherche par grille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8ca0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Jeux de données\n",
    "    \"train_user_article_ratings\": \"train_user_article_ratings\",\n",
    "    \"valid_user_article_ratings\": \"valid_user_article_ratings\",\n",
    "    \"article_profiles\": \"article_profiles\",\n",
    "    \n",
    "    # Hyperparamètres\n",
    "    \"rating_col\": DEFAULT_RATING_COL\n",
    "}\n",
    "\n",
    "gs_params = {\n",
    "    \"baseline_type\": [\"most_recent\", \"most_popular\", \"random\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a03890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from P9_02_scripts.model_baseline_train.run import exp_submit\n",
    "\n",
    "# On lance la recherche des hyperparamètres\n",
    "run = exp_submit(\n",
    "    ws,\n",
    "    SCRIPTS_PATH + \"model_baseline_train\",\n",
    "    params,\n",
    "    gs_params=gs_params,\n",
    "    wait_for_completion=False,\n",
    "    show_output=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec38233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # On affiche un widget avec les détails de l'exécution\n",
    "# RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106199d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On attend la fin de l'exécution\n",
    "run.wait_for_completion(show_output=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0dbc54",
   "metadata": {},
   "source": [
    "### Analyse des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f553ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère la dernière exécution de l'expérience.\n",
    "run = get_last_run(ws, \"model_baseline_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc50b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère la meilleure exécution de la recherche des hyperparamètres.\n",
    "best_run = run.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les hyperparamètres du meilleur modèle\n",
    "best_hyperparameters = json.loads(run.get_hyperparameters()[best_run.id])\n",
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed94fdbc",
   "metadata": {},
   "source": [
    "On obtient le meilleur Recall@5 en recommandant 5 articles au hasard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6776c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On télécharge le fichier contenant les résultats de l'évaluation du modèle\n",
    "best_run.download_file(\"outputs/res.parquet\", PARQUET_PATH + \"model_baseline_train_res.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf60307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ouvre et on affiche les résultats\n",
    "res = pd.read_parquet(PARQUET_PATH + \"model_baseline_train_res.parquet\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486be3d8",
   "metadata": {},
   "source": [
    "On obtient un Recall@5 très faible de 0.049 sur le jeu de validation.\n",
    "\n",
    "Sur toutes les recommandations faites aux utilisateurs, notre système n'a fait que 4.9% de recommandations pertinantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa077b9",
   "metadata": {},
   "source": [
    "### Enregistrement des hyperparamètres\n",
    "\n",
    "Nous allons enregistrer les meilleurs hyperparamètres dans le fichier `params.json` du dossier contenant les scripts d'entrainement du modèle. Ce fichier sera sauvegardé dans le repository Github et pourra être utilisé lors du déploiement automatisé du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525cbae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met en forme les paramètres\n",
    "best_hyperparameters = {k.replace(\"--\", \"\"): v for k, v in best_hyperparameters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met à jour les paramètres de base avec les meilleurs hyperparameters\n",
    "params.update(best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enregistre les paramètres\n",
    "with open(\"../P9_02_scripts/model_baseline_train/params.json\", \"w\") as f:\n",
    "    json.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68505d64",
   "metadata": {},
   "source": [
    "## Modèle de type content based\n",
    "\n",
    "Les modèles de type content based utilisent les features des articles pour recommander d'autres articles similaires à ceux déjà appréciés par l'utilisateurs.\n",
    "\n",
    "<img src=\"./data/img/content_based_filtering.png\" alt=\"Principe d’un modèle de type content based\" style=\"width:400px;\"/>\n",
    "<p style=\"text-align: center; text-decoration: underline;\">Principe d’un modèle de type content based</p>\n",
    "\n",
    "Nous allons tester un modèle très simple qui va utiliser le score de cosine similarity ([en savoir plus](https://en.wikipedia.org/wiki/Cosine_similarity)) pour trouver les articles ayant les profils les plus proches de celui de l'article moyen de l'utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f60b21",
   "metadata": {},
   "source": [
    "### Cold start problem\n",
    "\n",
    "Notre modèle ne sera confronté au cold start problem que pour les nouveaux utilisateurs. En effet, lorsque l'on va ajouter un nouvel article, celui-ci aura déjà assez d'information dans son profil pour que l'on puisse calculer des scores de similarité.\n",
    "\n",
    "Pour ce MVP, nous allons faire simple et utiliser les recommandations de notre baseline si l'utilisateur est inconnu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1fc05d",
   "metadata": {},
   "source": [
    "### Recherche des hyperparamètres\n",
    "\n",
    "Nous allons tester différentes pondérations/sélections des features qui seront utilisées lors du calcul des scores de similarité.\n",
    "\n",
    "Nous allons tester toutes les combinaisons de manière exhaustive en utilisant une recherche par grille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Jeux de données\n",
    "    \"train_user_article_ratings\": \"train_user_article_ratings\",\n",
    "    \"valid_user_article_ratings\": \"valid_user_article_ratings\",\n",
    "    \"article_profiles\": \"article_profiles\",\n",
    "    \"train_user_profiles\": \"train_user_profiles\",\n",
    "    \n",
    "    # Hyperparamètres\n",
    "    \"rating_col\": DEFAULT_RATING_COL\n",
    "}\n",
    "\n",
    "gs_params = {\n",
    "    \"num_vars_scale\": [0., 0.5, 1.],\n",
    "    \"cat_vars_scale\": [0., 0.5, 1.]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from P9_02_scripts.model_content_based_train.run import exp_submit\n",
    "\n",
    "# On lance la recherche des hyperparamètres\n",
    "run = exp_submit(\n",
    "    ws,\n",
    "    SCRIPTS_PATH + \"model_content_based_train\",\n",
    "    params,\n",
    "    gs_params=gs_params,\n",
    "    wait_for_completion=False,\n",
    "    show_output=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fba58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # On affiche un widget avec les détails de l'exécution\n",
    "# RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e15996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On attend la fin de l'exécution\n",
    "run.wait_for_completion(show_output=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1e3f9",
   "metadata": {},
   "source": [
    "### Analyse des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4fda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère la dernière exécution de l'expérience.\n",
    "run = get_last_run(ws, \"model_content_based_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d3ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère la meilleure exécution de la recherche des hyperparamètres.\n",
    "best_run = run.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0479ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les hyperparamètres du meilleur modèle\n",
    "best_hyperparameters = json.loads(run.get_hyperparameters()[best_run.id])\n",
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f7365",
   "metadata": {},
   "source": [
    "On obtient le meilleur Recall@5 en utilisant les embeddings ainsi que les catégories des articles dans la recherche de contenus pertinents pour l'utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74992751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On télécharge le fichier contenant les résultats de l'évaluation du modèle\n",
    "best_run.download_file(\"outputs/res.parquet\", PARQUET_PATH + \"model_content_based_train_res.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ouvre et on affiche les résultats\n",
    "res = pd.read_parquet(PARQUET_PATH + \"model_content_based_train_res.parquet\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6e088",
   "metadata": {},
   "source": [
    "On obtient un Recall@5 de 0.172 sur le jeu de validation.\n",
    "\n",
    "Sur toutes les recommandations faites aux utilisateurs, notre système a fait 17% de recommandations pertinantes. Ce score est bien meilleur que celui de notre baseline !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9f406",
   "metadata": {},
   "source": [
    "### Enregistrement des hyperparamètres\n",
    "\n",
    "Nous allons enregistrer les meilleurs hyperparamètres dans le fichier `params.json` du dossier contenant les scripts d'entrainement du modèle. Ce fichier sera sauvegardé dans le repository Github et pourra être utilisé lors du déploiement automatisé du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met en forme les paramètres\n",
    "best_hyperparameters = {k.replace(\"--\", \"\"): v for k, v in best_hyperparameters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df2ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met à jour les paramètres de base avec les meilleurs hyperparameters\n",
    "params.update(best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enregistre les paramètres\n",
    "with open(\"../P9_02_scripts/model_content_based_train/params.json\", \"w\") as f:\n",
    "    json.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d970a48",
   "metadata": {},
   "source": [
    "## Modèle de collaborative filtering\n",
    "\n",
    "Il faut imaginer une matrice avec 1 utilisateur par ligne et 1 artile par colonne. Une case va donc représenter la note atribuée par 1 utilisateur à 1 article. Le but du modèle va être de déduire les notes des articles jamais cliqués par l'utilisateur en se basant sur ces précédentes notations et sur les notations des utilisateurs similaires. On pourra ainsi recommander à l'utilisateur les articles dont le modèle estime que l'utilsateur les aurait bien noté.\n",
    "\n",
    "<img src=\"./data/img/collaborative_filtering.png\" alt=\"Principe d’un modèle de type collaborative filtering\" style=\"width:400px;\"/>\n",
    "<p style=\"text-align: center; text-decoration: underline;\">Principe d’un modèle de type collaborative filtering</p>\n",
    "\n",
    "Nous allons tester l'algorithme SVD de factorisation de matrice. On utilisera l'implémentation de la librairie Surprise ([en savoir plus](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD)). Cette librairie propose beaucoup d'autres algorithmes. Mais une fois de plus, ce projet a pour but de créer rapidement un MVP fonctionnel et de poser les base d'une architecture MLOps. Cela reste cependant une piste intéressante pour une future version du MVP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd88bcc",
   "metadata": {},
   "source": [
    "### Cold start problem\n",
    "\n",
    "Les modèles de collaborative filtering sont confrontés au cold start problem pour les nouveaux utilisateurs et pour les nouveaux articles.\n",
    "\n",
    "Pour ce MVP, nous allons faire simple. Nous allons utiliser les recommandations de notre baseline si l'utilisateur est inconnu. Nous laisserons la librairie Surprise gérer la notation d'un article jamais vu par le modèle. Par défaut, elle utilise la valeur moyenne de toutes les notes du jeu d'entrainement ([en savoir plus](https://surprise.readthedocs.io/en/stable/algobase.html#surprise.prediction_algorithms.algo_base.AlgoBase.default_prediction))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010676ec",
   "metadata": {},
   "source": [
    "### Recherche des hyperparamètres\n",
    "\n",
    "Les modèles de collaborative filtering sont basés sur la notation des articles par les utilisateurs. Nous n'avons pas de note explicite dans notre jeu de données. Nous avons donc créé des notes inplicites lors de la transformation des données. Ces notes sont déduites à partir du nombre de clicks sur chaque article. Nous avons 2 notes différentes que nous allons tester ici :\n",
    "- `rating_click_nb` : Nombre total de clicks par un utilisateur sur un article.\n",
    "- `rating_click_per_session_ratio` : `rating_click_nb` normalisé par le nombre total de clicks dans la session de l'utilisateur.\n",
    "\n",
    "Nous allons aussi tester d'autres hyperparamètres spécifiques à l'algorithme SVD de la librairie Surprise.\n",
    "\n",
    "Il y a beaucoup de combinaisons différentes des hyperparamètres. Nous allons utiliser une recherche avec sélection aléatoire de 50 combinaisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Jeux de données\n",
    "    \"train_user_article_ratings\": \"train_user_article_ratings\",\n",
    "    \"valid_user_article_ratings\": \"valid_user_article_ratings\",\n",
    "    \"article_profiles\": \"article_profiles\"\n",
    "}\n",
    "\n",
    "gs_params = {\n",
    "    \"rating_col\": [\"rating_click_nb\", \"rating_click_per_session_ratio\"],\n",
    "    \"n_factors\": [50, 100, 200],\n",
    "    \"n_epochs\": [10, 20, 30],\n",
    "    \"lr_all\": [0.0001, 0.005, 0.025],\n",
    "    \"reg_all\": [0.004, 0.02, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77be304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from P9_02_scripts.model_collaborative_filtering_train.run import exp_submit\n",
    "\n",
    "# On lance la recherche des hyperparamètres\n",
    "run = exp_submit(\n",
    "    ws,\n",
    "    SCRIPTS_PATH + \"model_collaborative_filtering_train\",\n",
    "    params,\n",
    "    gs_params=gs_params,\n",
    "    wait_for_completion=False,\n",
    "    show_output=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # On affiche un widget avec les détails de l'exécution\n",
    "# RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39285192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On attend la fin de l'exécution\n",
    "run.wait_for_completion(show_output=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61780f71",
   "metadata": {},
   "source": [
    "### Analyse des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère la dernière exécution de l'expérience.\n",
    "run = get_last_run(ws, \"model_collaborative_filtering_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f6178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère la meilleure exécution de la recherche des hyperparamètres.\n",
    "best_run = run.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les hyperparamètres du meilleur modèle\n",
    "best_hyperparameters = json.loads(run.get_hyperparameters()[best_run.id])\n",
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On télécharge le fichier contenant les résultats de l'évaluation du modèle\n",
    "best_run.download_file(\"outputs/res.parquet\", PARQUET_PATH + \"model_collaborative_filtering_train_res.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ouvre et on affiche les résultats\n",
    "res = pd.read_parquet(PARQUET_PATH + \"model_collaborative_filtering_train_res.parquet\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071c0782",
   "metadata": {},
   "source": [
    "On obtient un Recall@5 très faible de 0.049 sur le jeu de validation.\n",
    "\n",
    "Sur toutes les recommandations faites aux utilisateurs, notre système n'a fait que 4.9% de recommandations pertinantes. \n",
    "\n",
    "Ce modèle ne fait donc pas mieux que la baseline. Cela pourrait être du au fait que ce modèle fasses des recommandations trop variées. Les utilisateurs de ce jeu de données sont peut-être plus intéressés par les mêmes types de contenu, ce qui expliquerait les bien meilleurs résultats obtenus avec le modèle de type content based.\n",
    "\n",
    "Une autre hypothèse pourrait être que notre système de notation inplicite des articles n'est pas assez représentatif de la pertinance ou non d'un article du point de vu de l'utilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fe7b1",
   "metadata": {},
   "source": [
    "### Enregistrement des hyperparamètres\n",
    "\n",
    "Nous allons enregistrer les meilleurs hyperparamètres dans le fichier `params.json` du dossier contenant les scripts d'entrainement du modèle. Ce fichier sera sauvegardé dans le repository Github et pourra être utilisé lors du déploiement automatisé du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab97929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met en forme les paramètres\n",
    "best_hyperparameters = {k.replace(\"--\", \"\"): v for k, v in best_hyperparameters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f96a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met à jour les paramètres de base avec les meilleurs hyperparameters\n",
    "params.update(best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d66c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enregistre les paramètres\n",
    "with open(\"../P9_02_scripts/model_collaborative_filtering_train/params.json\", \"w\") as f:\n",
    "    json.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf1f47",
   "metadata": {},
   "source": [
    "# Sélection du meilleur modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe26b977",
   "metadata": {},
   "source": [
    "## Comparaison des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85938785",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_baseline_train_res = pd.read_parquet(PARQUET_PATH + \"model_baseline_train_res.parquet\")\n",
    "model_content_based_train_res = pd.read_parquet(PARQUET_PATH + \"model_content_based_train_res.parquet\")\n",
    "model_collaborative_filtering_train_res = pd.read_parquet(PARQUET_PATH + \"model_collaborative_filtering_train_res.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On réunit tous les résultats\n",
    "valid_res = pd.concat([\n",
    "    model_baseline_train_res,\n",
    "    model_content_based_train_res,\n",
    "    model_collaborative_filtering_train_res\n",
    "])\n",
    "\n",
    "# On classe les modèles en fonction du meilleur recall obtenu sur le jeu de validation\n",
    "valid_res = valid_res.sort_values(\"recall@5\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# On modifie le nom des colonnes\n",
    "valid_res = valid_res.rename(columns={\n",
    "    \"precision@5\": \"valid_precision@5\",\n",
    "    \"recall@5\": \"valid_recall@5\",\n",
    "})\n",
    "valid_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a062e",
   "metadata": {},
   "source": [
    "On obtient le meilleur Recall@5 sur le jeu de validation avec le modèle de type content based."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdcdaf7",
   "metadata": {},
   "source": [
    "## Enregistrement du modèle\n",
    "\n",
    "Nous allons enregistrer manuellement notre meilleur modèle afin de pouvoir l'évaluer et le déployer dans ce notebook.\n",
    "\n",
    "En production, cette étape sera réalisée automatiquement après l'entrainement du modèle dans le pipeline de déploiement automatisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd496a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère la dernière exécution de l'expérience.\n",
    "run = get_last_run(ws, \"model_content_based_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère la meilleure exécution de la recherche des hyperparamètres.\n",
    "best_run = run.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926fb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enregistre le modèle\n",
    "model = best_run.register_model(\n",
    "    model_name=\"recommender\",\n",
    "    model_path=\"outputs/model.joblib\",\n",
    "    tags={\"class_name\": \"ContentBasedRecommender\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da63180c",
   "metadata": {},
   "source": [
    "# Analyse du modèle\n",
    "\n",
    "Avant de déployer notre modèle, nous allons l'évaluer sur le jeu de test afin de vérifier qu'il généralise correctement sur des données qu'il n'a jamais vu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074ee12f",
   "metadata": {},
   "source": [
    "## Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640297f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On télécharge les données\n",
    "model = Model(ws, \"recommender\")\n",
    "model_path = model.download(target_dir=MODEL_PATH, exist_ok=True)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d892ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from P9_02_scripts import models\n",
    "\n",
    "# On inscrit les modules nécessaires au chargement du modèle\n",
    "sys.modules['models'] = models\n",
    "\n",
    "# On charge le modèle\n",
    "model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7b121",
   "metadata": {},
   "source": [
    "## Chargement des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40536b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge les datasets\n",
    "test_user_article_ratings_ds = Dataset.get_by_name(ws, \"test_user_article_ratings\")\n",
    "article_profiles_ds = Dataset.get_by_name(ws, \"article_profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676dcb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge les datasets dans des DataFrames\n",
    "test_user_article_ratings = test_user_article_ratings_ds.to_pandas_dataframe()\n",
    "article_profiles = article_profiles_ds.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec2acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On filtre les colonnes pour obtenir le format : user_id, article_id, rating_id\n",
    "test_ratings = test_user_article_ratings[[\"user_id\", \"article_id\", DEFAULT_RATING_COL]]\n",
    "test_ratings = test_ratings.rename(columns={DEFAULT_RATING_COL: \"rating\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e468a3",
   "metadata": {},
   "source": [
    "## Evaluation sur le jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8fc160",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = get_precision_recall_n_score(\n",
    "    model,\n",
    "    \"ContentBasedRecommender\",\n",
    "    test_ratings,\n",
    "    article_profiles,\n",
    "    top_n=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On recomme les colonnes\n",
    "test_res = test_res.rename(columns={\n",
    "    \"precision@5\": \"test_precision@5\",\n",
    "    \"recall@5\": \"test_recall@5\",\n",
    "})\n",
    "\n",
    "# On merge les résultats avec ceux du jeu de validation\n",
    "pd.merge(valid_res, test_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2802ab75",
   "metadata": {},
   "source": [
    "On obtient sur le jeu de test un Recall@5 de 0.193.\n",
    "\n",
    "Ce score est similaire, voir même supérieur à celui obtenu sur le jeu de validation (0.172). Cela semble indiquer que notre modèle généralise correctement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0d41d84",
   "metadata": {},
   "source": [
    "# Déploiement du MVP\n",
    "\n",
    "Nous allons maintenant déployer notre modèle ainsi que le service de recommandation. Nous allons ainsi créer les blocs `Model service` et `Recommendation service`.\n",
    "\n",
    "<img src=\"./data/img/archi_prod.png\" alt=\"Mise en production et exploitation\" width=\"900\"/>\n",
    "<p style=\"text-align: center; text-decoration: underline;\">Mise en production et exploitation</p>\n",
    "\n",
    "Dans ce notebook, nous allons réaliser ces opérations manuellement. En production, on pourra lancer le pipeline de déploiement [Train and deploy](https://github.com/Sako74/p9-demo/actions/workflows/train_deploy.yml). Ce dernier va réaliser les étapes suivantes :\n",
    "- Entrainer le modèle avec les meilleurs hyperparamètres trouvés précédemment.\n",
    "- Enregistrer le modèle entrainé.\n",
    "- Déployer le modèle sur ACI.\n",
    "- Déployer le service de recommandation via une Azure function.\n",
    "\n",
    "<img src=\"./data/img/train_and_deploy_pipeline.png\" alt=\"Pipeline de déploiement\" width=\"1000\"/>\n",
    "<p style=\"text-align: center; text-decoration: underline;\">Résultats du pipeline de déploiement du système de recommandation via les Github actions.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53f57b",
   "metadata": {},
   "source": [
    "## Déploiement du modèle sur Azure Container Instances\n",
    "\n",
    "Nous allons utiliser Azure Container Instances. Azure va automatiquement créer un service web et un point de terminaison pour y accéder via une API REST. Il s'agit d'une solution simple pour déployer un modèle à titre expérimental comme dans le cadre de ce MVP. Cette solution est cependant déconseillé par Microsoft pour le déploiement de modèle en production. Microsoft recommande plutôt de déployer un modèle en tant que service web sur Azure Kubernetes service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe9630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from P9_02_scripts.model_content_based_deploy_aci.run import model_deploy\n",
    "\n",
    "# On déploie le modèle sur ACI.\n",
    "# Cette fonction va aussi créer un fichier \".env\"\n",
    "# qui va contenir l'URL de l'API REST déployée.\n",
    "model_aci = model_deploy(\n",
    "    ws,\n",
    "    SCRIPTS_PATH + \"model_content_based_deploy_aci\",\n",
    "    show_output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3cf627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche l'URL de l'API REST. \n",
    "print(model_aci.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c71b77",
   "metadata": {},
   "source": [
    "## Test du modèle\n",
    "\n",
    "Nous allons tester manuellement le modèle déployé. A terme, il serait intéressant d'automatiser cette étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc87bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donnée à envoyer\n",
    "data = {\n",
    "    \"user_id\": \"1234\",\n",
    "    \"session_start_dt\": datetime(2017, 10, 16, 12).isoformat(),\n",
    "    \"top_n\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a193f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On envoie la requête\n",
    "r = requests.post(model_aci.scoring_uri, json=data)\n",
    "\n",
    "# On vérifie le code de la réponse\n",
    "if not r.ok:\n",
    "    print(f\"Erreur de type {r.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab26f6cf",
   "metadata": {},
   "source": [
    "Le service web nous a bien renvoyée les ids des 5 articles recommandés par le modèle (champ `article_ids`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5431ddc",
   "metadata": {},
   "source": [
    "## Déploiement du service de prédiction\n",
    "\n",
    "Le service de prédiction sert d’interface entre le/les modèles déployés et l’application mobile. Nous avons utilisé le service Azure function pour développer ce service. Azure Functions est une solution serverless qui permet de déployer des services qui répondent à des évènements (requête HTTP, timer etc...) sans se préocuper de l'infrastructure qui va les faire tourner.\n",
    "\n",
    "Dans ce notebook, nous allons utiliser des scripts `bash` qui permettent de créer les ressources Azure et de déployer le code du service de recommandation.\n",
    "\n",
    "Le pipeline Github [Train and deploy](https://github.com/Sako74/p9-demo/actions/workflows/train_deploy.yml) va quant à lui utiliser une Action développée par Mircosoft Azure pour déployer le code. Pour utiliser cette solution, veuillez suivre les insctructions du fichier `REAMDE.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9661d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = SCRIPTS_PATH + \"model_content_based_deploy_aci/.env\"\n",
    "dst = FUNCTION_PATH + \"recommender/.env\"\n",
    "\n",
    "# On crée le fichier .env dans le dossier de l'azure function\n",
    "copy2(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d149687",
   "metadata": {},
   "source": [
    "Création et déploiement de l'azure function :\n",
    "- `conda activate p9`\n",
    "- `cd P9_03_function/`\n",
    "- `./function_create.sh`\n",
    "- `./function_deploy.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a82ad5",
   "metadata": {},
   "source": [
    "## Test du service de recommandation\n",
    "\n",
    "Nous allons tester manuellement le service de recommandation. A terme, il serait intéressant d'automatiser cette étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f9767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On lit les configurations\n",
    "config_parser = ConfigParser()\n",
    "config_parser.read(FUNCTION_PATH + \"function_config.txt\")\n",
    "\n",
    "# On récupère le nom du site\n",
    "azure_function_name = config_parser.get(\"DEFAULT\", \"functionAppName\").strip('\"')\n",
    "print(f\"Le nom de l'azure function est : {azure_function_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a21eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de données qui sera envoyée par l'application mobile\n",
    "data = {\"userId\": \"1234\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On envoie la requête\n",
    "r = requests.post(f\"https://{azure_function_name}.azurewebsites.net/api/recommender\", json=data)\n",
    "\n",
    "# On vérifie le code de la réponse\n",
    "if not r.ok:\n",
    "    print(f\"Erreur de type {r.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea29ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d4dedf",
   "metadata": {},
   "source": [
    "L'Azure function nous a bien renvoyée la liste des 5 ids des articles recommandés par le modèle. Ce format a été spécifié par le prestataire qui a réalisé l'application mobile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b0d74",
   "metadata": {},
   "source": [
    "## Test du MVP\n",
    "\n",
    "<img src=\"./data/gif/mvp_demo.gif\" alt=\"Démonstration MVP\" style=\"width:900px;\"/>\n",
    "<p style=\"text-align: center; text-decoration: underline;\">Démonstration du MVP</p>\n",
    "\n",
    "On peut voir ci-dessus le test du MVP.\n",
    "\n",
    "L'application mobile a été lancée sur un smartphone émulé sur ordinateur. Pour réaliser ce test, nous avons suivit les instructions du document suivant : [Mode opératoire de test de l’Azure function](https://s3.eu-west-1.amazonaws.com/course.oc-static.com/projects/Ing%C3%A9nieur_IA_P9/Mode+ope%CC%81ratoire+test+Azure+function_V1.1.docx.pdf).\n",
    "\n",
    "La fenêtre du haut affiche les logs du bloc `Model service`. Il faut attendre la fin de l'animation pour observer les logs car il faut mettre à jour cette fenêtre manuellement. On pourra y observer les prédictions du modèle.\n",
    "\n",
    "La fenêtre du bas affiche les logs du bloc `Recommender service`. On pourra y observer les prédictions du système de recommandation.\n",
    "\n",
    "Ces logs pourront par la suite êtres extraits par les blocs `Monitoring` et `ETL` pour êtres enregistrés dans le `Feature store`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14547cc1",
   "metadata": {},
   "source": [
    "## Nettoyage des ressources\n",
    "\n",
    "On pensera à supprimer le service web à la fin de cette démonstration pour éviter des coût inutiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e25eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère le endpoint du modèle déployé\n",
    "model_aci = Webservice(ws, \"p9-recommender-aci\")\n",
    "\n",
    "# On supprime le endpoint\n",
    "model_aci.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25d0ac",
   "metadata": {},
   "source": [
    "Suppression du groupe de ressources du service de prédiction\n",
    "- `conda activate p9`\n",
    "- `cd P9_03_function/`\n",
    "- `./function_delete.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b7c55",
   "metadata": {},
   "source": [
    "# Architecture de déploiement automatisé\n",
    "\n",
    "<img src=\"./data/images/MLOps level 1.svg\" alt=\"MLOps level 1.svg\" width=\"900\"/>\n",
    "<p style=\"text-align: center; text-decoration: underline;\">MLOps level 1 (<a href=\"https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning\">source</a>)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bc3dbc",
   "metadata": {},
   "source": [
    "# Pour aller plus loin\n",
    "\n",
    "## Compléter les blocs manquants de l'architecture MLOps\n",
    "\n",
    "Il serait intéressant de compléter les blocs manquants de l'architecture. En effet, si notre MVP est concluant, il sera alors plus rapide d'itérer et de créer les futures versions du MVP.\n",
    "\n",
    "### Acquisition des données\n",
    "\n",
    "<img src=\"./data/img/archi_data_collect.png\" alt=\"Acquisition des données\" width=\"400\"/>\n",
    "<p style=\"text-align: center; text-decoration: underline;\">Acquisition des données</p>\n",
    "\n",
    "Afin de pouvoir faire l'acquisition de notre propres données, il nous manquerait les blocs suivants :\n",
    "- `Dashboard` :\n",
    "    - Permet l’ajout de nouveaux articles.\n",
    "- `App backend` :\n",
    "    - Enregistre les évènements de l’application mobile dans des logs.\n",
    "    - Permet l’ajout de nouveaux utilisateurs.\n",
    "    - Permet l’ajout de nouveaux articles.\n",
    "- `ETL` :\n",
    "    - Azure function qui s’exécute par exemple 1 fois par jour.\n",
    "    - Extract, Transform and Load des données du backend vers le feature store.\n",
    "\n",
    "### Monitoring des données\n",
    "\n",
    "<img src=\"./data/img/archi_monitoring.png\" alt=\"Monitoring des données\" width=\"900\"/>\n",
    "<p style=\"text-align: center; text-decoration: underline;\">Monitoring des données</p>\n",
    "\n",
    "Afin de pouvoir effectuer le monitoring de nos données, il nous manquerait les blocs/fonctionnalités suivantes :\n",
    "- `Monitoring` :\n",
    "\t- Surveillances de la santé des services déployés.\n",
    "\t- Enregistrement des prédictions des modèles.\n",
    "\n",
    "- `ETL` :\n",
    "\t- Azure function qui s’exécute par exemple 1 fois par jour.\n",
    "\t- Extract, Transform and Load des données du monitoring vers le feature store.\n",
    "\n",
    "- `Data analysis` :\n",
    "\t- Agrégation des données du backend et du monitoring.\n",
    "\t- Calcul du taux de click des articles recommandés.\n",
    "\n",
    "- `Trigger` :\n",
    "\t- Azure function.\n",
    "\t- Détection de data drift.\n",
    "\t- Détection de model drift.\n",
    "\t- Lancement automatique de pipeline de déploiement.\n",
    "\n",
    "## Ajout et automatisation des tests\n",
    "\n",
    "Il serait aussi intéressant d'ajouter et d'automatiser les tests :\n",
    "- Unitaires.\n",
    "- Intégration.\n",
    "- Smoke test.\n",
    "- Validation des données.\n",
    "- etc...\n",
    "\n",
    "## Analyse de la stabilité temporelle\n",
    "\n",
    "Nous pourrions analyser la stabilité temporelle du modèle. Cela nous permettrait d'estimer la fréquence et/ou les triggers qui pourraient déclencher le ré-entrainement du modèle avec de nouvelles données. On pourait par exemple constater qu'il faudrait ré-entrainer le modèle :\n",
    "- Une fois par jour.\n",
    "- Au bout de 10% de nouveaux utilisateurs.\n",
    "- Etc...\n",
    "\n",
    "\n",
    "## Pousser l'expérimentation ML\n",
    "\n",
    "Pour améliorer notre système de recommandation, il faudrait aussi tester d'autres modèles et librairies.\n",
    "- Librairie Microsoft Recommender ([en savoir plus](https://github.com/microsoft/recommenders/)).\n",
    "- Modèles hybrides ([en savoir plus](https://www.youtube.com/watch?v=EgE0DUrYmo8)).\n",
    "- Modèles de gradient boosting ([en savoir plus](https://www.youtube.com/watch?v=kI6VbkrIbpg)).\n",
    "- Etc...\n",
    "\n",
    "Nous avons travaillé avec une fenêtre temporelle fixe pour le jeu d'entrainement et le jeu de validation. Nous pourrions faire de la validation croisée en déplaçant ces fenêtres temporelles sur l'ensemble du jeu de données. Cela nous permettrait d'obtenir des évaluations plus robustes.\n",
    "\n",
    "Enfin il serait intéressant de monitorer d'autres métriques.\n",
    "- Temps de session moyen.\n",
    "- Observer la diversité de prédictions.\n",
    "- Etc..."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "p9"
  },
  "kernelspec": {
   "display_name": "p9",
   "language": "python",
   "name": "p9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Sommaire",
   "title_sidebar": "Sommaire",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "351px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
